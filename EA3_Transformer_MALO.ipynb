{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ffe8d7c0",
      "metadata": {
        "id": "ffe8d7c0"
      },
      "source": [
        "# Evaluación Parcial 3 – Modelo Transformer\n",
        "\n",
        "\n",
        "Este cuaderno desarrolla un **modelo Transformer** para la generación de respuestas en diálogos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bacf14",
      "metadata": {
        "id": "a8bacf14"
      },
      "source": [
        "## 1 | Introducción\n",
        "\n",
        "El objetivo es enseñar a un modelo a **predecir la respuesta** a una intervención dentro de un diálogo. Se utilizará la columna `dialog` de un conjunto de datos proporcionado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279b9eb9",
      "metadata": {
        "id": "279b9eb9"
      },
      "outputs": [],
      "source": [
        "# --- Librerías principales\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import pprint\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# reproducibilidad\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ec7064",
      "metadata": {
        "id": "75ec7064"
      },
      "source": [
        "## 2 | Carga y exploración de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17507126",
      "metadata": {
        "id": "17507126"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/data/dialog/train.csv\n",
        "!wget -q https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/data/dialog/validation.csv\n",
        "!wget -q https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/data/dialog/test.csv\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "valid_df = pd.read_csv(\"validation.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1879fa9",
      "metadata": {
        "id": "d1879fa9"
      },
      "source": [
        "## 3 | Preprocesamiento de diálogos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1eda14f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1eda14f",
        "outputId": "53094a04-7249-4a60-dd99-f2388a5a5fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pares totales extraídos: 35,450\n",
            "Q: ['Say , Jim , how about going for a few beers after dinner ? '\n",
            "A: ' You know that is tempting but is really not good for our fitness . '\n",
            "----------------------------------------\n",
            "Q: ' What do you mean ? It will help us to relax . '\n",
            "A: \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \"\n",
            "----------------------------------------\n",
            "Q: \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \"\n",
            "A: ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . '\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def extract_pairs(text: str):\n",
        "    turns = [l.strip() for l in text.split('\\n') if l.strip()]\n",
        "    pairs = []\n",
        "    for i in range(0, len(turns)-1, 2):\n",
        "        q, a = turns[i], turns[i+1]\n",
        "        if q and a:\n",
        "            pairs.append((q, a))\n",
        "    return pairs\n",
        "\n",
        "pairs = []\n",
        "for d in train_df['dialog'].astype(str):\n",
        "    pairs.extend(extract_pairs(d))\n",
        "\n",
        "print(f'Pares totales extraídos: {len(pairs):,}')\n",
        "\n",
        "# Mostrar ejemplo\n",
        "for q,a in pairs[:3]:\n",
        "    print('Q:', q)\n",
        "    print('A:', a)\n",
        "    print('-'*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c1f1cf9",
      "metadata": {
        "id": "1c1f1cf9"
      },
      "source": [
        "### 3.1 | División Train / Val / Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c530a27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c530a27",
        "outputId": "dd08a0bc-6e56-4886-e3ed-66679911172e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 28360 | Val: 3545 | Test: 3545\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(pairs)\n",
        "total = len(pairs)\n",
        "train_cut = int(0.8*total)\n",
        "val_cut   = int(0.9*total)\n",
        "\n",
        "train_pairs = pairs[:train_cut]\n",
        "val_pairs   = pairs[train_cut:val_cut]\n",
        "test_pairs  = pairs[val_cut:]\n",
        "\n",
        "print(f'Train: {len(train_pairs)} | Val: {len(val_pairs)} | Test: {len(test_pairs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa26234",
      "metadata": {
        "id": "4fa26234"
      },
      "source": [
        "## 4 | Vectorización de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e7d58e",
      "metadata": {
        "id": "29e7d58e"
      },
      "outputs": [],
      "source": [
        "MAX_LEN   = 50   # según rúbrica y análisis exploratorio\n",
        "MIN_FREQ  = 1\n",
        "BATCH_SZ  = 64\n",
        "\n",
        "special_new = ['<bos>', '<eos>']   # solo los que añades\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_LEN\n",
        ")\n",
        "\n",
        "# 1) recopilar todos los textos (pregunta + respuesta)\n",
        "all_text = [txt for q, a in (train_pairs + val_pairs + test_pairs) for txt in (q, a)]\n",
        "print(\"Textos totales:\", len(all_text))\n",
        "\n",
        "# 2) adaptar → crea vocabulario base con '' y '[UNK]' al frente\n",
        "vectorizer.adapt(all_text)\n",
        "\n",
        "# 3) construir vocabulario final:\n",
        "vocab_base = vectorizer.get_vocabulary()[2:]        # sin '' ni [UNK]\n",
        "new_vocab  = ['', '[UNK]'] + special_new + vocab_base\n",
        "vectorizer.set_vocabulary(new_vocab)\n",
        "\n",
        "# 4) IDs útiles\n",
        "PAD_ID, UNK_ID = 0, 1\n",
        "BOS_ID, EOS_ID = 2, 3\n",
        "VOCAB_SIZE = vectorizer.vocabulary_size()\n",
        "print(\"VOCAB_SIZE:\", VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3103afe",
      "metadata": {
        "id": "a3103afe"
      },
      "source": [
        "### 4.1 | Creación de objetos `tf.data.Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11819b96",
      "metadata": {
        "id": "11819b96"
      },
      "outputs": [],
      "source": [
        "def format_dataset(pairs):\n",
        "    q_texts = [q for q,_ in pairs]\n",
        "    a_texts = [a for _,a in pairs]\n",
        "\n",
        "    enc = tf.cast(vectorizer(q_texts), tf.int32)\n",
        "    dec_in  = tf.cast(vectorizer(['<bos> '+t for t in a_texts]), tf.int32)\n",
        "    dec_out = tf.cast(vectorizer([t+' <eos>' for t in a_texts]), tf.int32)\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(((enc, dec_in), dec_out))\n",
        "\n",
        "\n",
        "def prepare_tf_dataset(pairs):\n",
        "    ds = format_dataset(pairs)\n",
        "    return (ds\n",
        "            .shuffle(10_000, seed=SEED)\n",
        "            .batch(BATCH_SZ)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "train_ds = prepare_tf_dataset(train_pairs)\n",
        "val_ds   = prepare_tf_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1a3f5f",
      "metadata": {
        "id": "6d1a3f5f"
      },
      "source": [
        "## 5 | Componentes del modelo Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf50715",
      "metadata": {
        "id": "dbf50715"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, max_len, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_encoding = positional_encoding(max_len, d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Si llega un SparseTensor lo convertimos\n",
        "        if isinstance(x, tf.SparseTensor):\n",
        "            x = tf.sparse.to_dense(x)\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        return x + self.pos_encoding[:, :seq_len, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280f1143",
      "metadata": {
        "id": "280f1143"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(num_layers, d_model, num_heads, dff, input_vocab, maximum_position_encoding, rate=0.1):\n",
        "    inputs   = layers.Input(shape=(None,), name='enc_input')\n",
        "    padding_mask = layers.Lambda(lambda x: tf.cast(tf.math.equal(x, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :])(inputs)\n",
        "\n",
        "    x = layers.Embedding(input_vocab, d_model)(inputs)\n",
        "    x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    x = PositionalEncoding(maximum_position_encoding, d_model)(x)\n",
        "    x = layers.Dropout(rate)(x)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        # multi‑head attention\n",
        "        attn_out = layers.MultiHeadAttention(num_heads, key_dim=d_model, dropout=rate)(x, x, attention_mask=padding_mask)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_out)\n",
        "\n",
        "        ffn_out = layers.Dense(dff, activation='relu')(x)\n",
        "        ffn_out = layers.Dense(d_model)(ffn_out)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_out)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x, name='encoder')\n",
        "\n",
        "def transformer_decoder(num_layers, d_model, num_heads, dff, target_vocab, maximum_position_encoding, rate=0.1):\n",
        "    inputs   = layers.Input(shape=(None,), name='dec_input')\n",
        "    enc_outs = layers.Input(shape=(None, d_model), name='enc_output')\n",
        "\n",
        "    look_ahead_mask = layers.Lambda(\n",
        "        lambda x: 1 - tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0))(inputs)\n",
        "    padding_mask = layers.Lambda(lambda x: tf.cast(tf.math.equal(x, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :])(inputs)\n",
        "\n",
        "    embed = layers.Embedding(target_vocab, d_model)(inputs)\n",
        "    embed *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embed = PositionalEncoding(maximum_position_encoding, d_model)(embed)\n",
        "    x = layers.Dropout(rate)(embed)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        attn1 = layers.MultiHeadAttention(num_heads, key_dim=d_model, dropout=rate)(x, x, attention_mask=look_ahead_mask)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn1)\n",
        "\n",
        "        attn2 = layers.MultiHeadAttention(num_heads, key_dim=d_model, dropout=rate)(x, enc_outs, enc_outs)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn2)\n",
        "\n",
        "        ffn = layers.Dense(dff, activation='relu')(x)\n",
        "        ffn = layers.Dense(d_model)(ffn)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn)\n",
        "\n",
        "    outputs = layers.Dense(target_vocab)(x)\n",
        "    return tf.keras.Model([inputs, enc_outs], outputs, name='decoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa9f2b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "8fa9f2b0",
        "outputId": "08c12dfd-2468-4f4d-abe3-0fadc7ccc52a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"seq2seq_transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │  \u001b[38;5;34m5,364,992\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m10,223,410\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m21298\u001b[0m)            │            │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,364,992</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,223,410</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">21298</span>)            │            │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,588,402\u001b[0m (59.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,588,402</span> (59.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,588,402\u001b[0m (59.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,588,402</span> (59.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "NUM_LAYERS = 4\n",
        "D_MODEL    = 128\n",
        "NUM_HEADS  = 8\n",
        "DFF        = 512\n",
        "DROPOUT    = 0.1\n",
        "\n",
        "encoder = transformer_encoder(NUM_LAYERS, D_MODEL, NUM_HEADS, DFF, VOCAB_SIZE, MAX_LEN, DROPOUT)\n",
        "decoder = transformer_decoder(NUM_LAYERS, D_MODEL, NUM_HEADS, DFF, VOCAB_SIZE, MAX_LEN, DROPOUT)\n",
        "\n",
        "enc_inputs  = layers.Input(shape=(None,), name='encoder_inputs')\n",
        "dec_inputs  = layers.Input(shape=(None,), name='decoder_inputs')\n",
        "\n",
        "enc_outs = encoder(enc_inputs)\n",
        "dec_outs = decoder([dec_inputs, enc_outs])\n",
        "\n",
        "model = tf.keras.Model([enc_inputs, dec_inputs], dec_outs, name='seq2seq_transformer')\n",
        "\n",
        "# Loss & metrics\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss_ = loss_object(y_true, y_pred)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    match = tf.cast(tf.equal(y_true, tf.cast(y_pred, tf.int32)), tf.float32)\n",
        "    mask  = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    match *= mask\n",
        "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=masked_loss, metrics=[masked_accuracy])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299e51ca",
      "metadata": {
        "id": "299e51ca"
      },
      "source": [
        "## 6 | Entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d26ac3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51d26ac3",
        "outputId": "296fff6a-c42b-4665-8347-6f328c7d847c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 211ms/step - loss: 8.2271 - masked_accuracy: 0.1286 - val_loss: 5.5856 - val_masked_accuracy: 0.2358\n",
            "Epoch 2/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 151ms/step - loss: 5.0350 - masked_accuracy: 0.3252 - val_loss: 3.2634 - val_masked_accuracy: 0.6081\n",
            "Epoch 3/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 149ms/step - loss: 2.9385 - masked_accuracy: 0.6371 - val_loss: 2.0650 - val_masked_accuracy: 0.7478\n",
            "Epoch 4/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 150ms/step - loss: 1.9620 - masked_accuracy: 0.7540 - val_loss: 1.5489 - val_masked_accuracy: 0.8087\n",
            "Epoch 5/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 148ms/step - loss: 1.5037 - masked_accuracy: 0.8107 - val_loss: 1.2424 - val_masked_accuracy: 0.8473\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e901667a",
      "metadata": {
        "id": "e901667a"
      },
      "source": [
        "## 7 | Decodificación y evaluación BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc46bbab",
      "metadata": {
        "id": "cc46bbab"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ─── IDs y vocabulario globales ──────────────────────────────────────────\n",
        "BOS_ID = vectorizer('<bos>').numpy()[0]\n",
        "EOS_ID = vectorizer('<eos>').numpy()[0]\n",
        "VOCAB  = vectorizer.get_vocabulary()          # lista idx→token\n",
        "\n",
        "# ─── Generación (greedy) ─────────────────────────────────────────────────\n",
        "def generate(model, src_text: str, max_len: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    Devuelve la secuencia generada por 'model' para 'src_text'.\n",
        "    \"\"\"\n",
        "    enc_in = vectorizer([src_text])               # (1, enc_len)\n",
        "    dec_in = tf.expand_dims([BOS_ID], 0)          # (1, 1)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits = model([enc_in, dec_in], training=False)  # (1, t, vocab)\n",
        "        next_id = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
        "        dec_in  = tf.concat([dec_in, tf.expand_dims(next_id, -1)], axis=-1)\n",
        "\n",
        "        if next_id[0] == EOS_ID:\n",
        "            break\n",
        "\n",
        "    # ids → tokens, eliminando <bos>/<eos>\n",
        "    ids = dec_in.numpy().squeeze()\n",
        "    tokens = [VOCAB[i] for i in ids if i not in (BOS_ID, EOS_ID)]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ─── Evaluación BLEU ─────────────────────────────────────────────────────\n",
        "def bleu_score(model,\n",
        "               pairs,\n",
        "               max_len: int = 50) -> float:\n",
        "    \"\"\"\n",
        "    Calcula corpus-BLEU (%) sobre una lista de pares (src, ref).\n",
        "    \"\"\"\n",
        "    references, candidates = [], []\n",
        "    for src, ref in pairs:\n",
        "        pred = generate(model, src, max_len=max_len)\n",
        "        references.append([ref.split()])      # lista de refs por oración\n",
        "        candidates.append(pred.split())\n",
        "\n",
        "    return corpus_bleu(references, candidates) * 100\n",
        "\n",
        "# ─── Ejemplo de uso ──────────────────────────────────────────────────────\n",
        "score = bleu_score(model, test_pairs, max_len=MAX_LEN)\n",
        "print(f\"BLEU en test: {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- obtener índices de tokens especiales\n",
        "BOS_ID = vectorizer('<bos>').numpy()[0]\n",
        "EOS_ID = vectorizer('<eos>').numpy()[0]\n",
        "PAD_ID = vectorizer('<pad>').numpy()[0]\n",
        "\n",
        "VOCAB  = vectorizer.get_vocabulary()      # lista → idx→token\n",
        "\n",
        "def generate_response(model,prompt: str,\n",
        "                      max_len: int = MAX_LEN,\n",
        "                      temperature: float = 0.0) -> str:\n",
        "    enc_input = vectorizer([prompt])\n",
        "    dec_input = tf.expand_dims([BOS_ID], 0)        # (1,1)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits = model([enc_input, dec_input], training=False)[:, -1, :]\n",
        "\n",
        "        if temperature == 0.0:\n",
        "            #  tf.argmax → (1,)  →  expand_dims → (1,1)\n",
        "            next_id = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            next_id = tf.expand_dims(next_id, -1)\n",
        "        else:\n",
        "            #  tf.random.categorical ya sale (1,1)\n",
        "            next_id = tf.random.categorical(logits / temperature,\n",
        "                                            num_samples=1,\n",
        "                                            dtype=tf.int32)\n",
        "\n",
        "        dec_input = tf.concat([dec_input, next_id], axis=-1)\n",
        "\n",
        "        if next_id[0, 0] == EOS_ID:\n",
        "            break\n",
        "\n",
        "    ids = dec_input.numpy().squeeze()\n",
        "    keep = [i for i in ids if i not in (BOS_ID, EOS_ID, PAD_ID)]\n",
        "    tokens = [VOCAB[i] for i in keep]\n",
        "    return \" \".join(tokens).strip()\n",
        "\n",
        "def chat(model, temperature: float = 0.0):\n",
        "    \"\"\"\n",
        "    Bucle interactivo de consola.\n",
        "    Escribe 'salir' para terminar.\n",
        "    \"\"\"\n",
        "    print(\"=== Chat Transformer (escribe 'salir' para terminar) ===\")\n",
        "    while True:\n",
        "        user = input(\"Tú: \").strip()\n",
        "        if user.lower() in {\"salir\", \"exit\", \"quit\"}:\n",
        "            print(\"Hasta luego 👋\")\n",
        "            break\n",
        "        bot = generate_response(model, user, temperature=temperature)\n",
        "        print(\"Bot:\", bot)"
      ],
      "metadata": {
        "id": "ZROLX-ljYkgK"
      },
      "id": "ZROLX-ljYkgK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6mNS8M-MMMoV",
        "outputId": "4c36e449-0d5a-4dd5-9852-e557adfabb7f"
      },
      "id": "6mNS8M-MMMoV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aLpxSApMHre",
        "outputId": "e92dde23-ddcb-478e-dd6f-939fc6f15912"
      },
      "id": "3aLpxSApMHre",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Chat Transformer (escribe 'salir' para terminar) ===\n",
            "Tú: hello\n",
            "Bot: when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when\n",
            "Tú: bye bye\n",
            "Bot: when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when\n",
            "Tú: salir\n",
            "Hasta luego 👋\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}