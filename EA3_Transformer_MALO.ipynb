{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ffe8d7c0",
      "metadata": {
        "id": "ffe8d7c0"
      },
      "source": [
        "# EvaluaciÃ³n ParcialÂ 3 â€“ ModeloÂ Transformer\n",
        "\n",
        "\n",
        "Este cuaderno desarrolla un **modelo Transformer** para la generaciÃ³n de respuestas en diÃ¡logos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bacf14",
      "metadata": {
        "id": "a8bacf14"
      },
      "source": [
        "## 1Â |Â IntroducciÃ³n\n",
        "\n",
        "El objetivo esÂ enseÃ±ar a un modelo a **predecir la respuesta** a una intervenciÃ³n dentro de un diÃ¡logo.Â Se utilizarÃ¡ la columna `dialog` de un conjunto de datos proporcionado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279b9eb9",
      "metadata": {
        "id": "279b9eb9"
      },
      "outputs": [],
      "source": [
        "# --- LibrerÃ­as principales\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import pprint\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# reproducibilidad\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ec7064",
      "metadata": {
        "id": "75ec7064"
      },
      "source": [
        "## 2Â |Â Carga y exploraciÃ³n de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17507126",
      "metadata": {
        "id": "17507126"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/data/dialog/train.csv\n",
        "!wget -q https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/data/dialog/validation.csv\n",
        "!wget -q https://raw.githubusercontent.com/JaznaLaProfe/Deep-Learning/main/data/dialog/test.csv\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "valid_df = pd.read_csv(\"validation.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1879fa9",
      "metadata": {
        "id": "d1879fa9"
      },
      "source": [
        "## 3Â |Â Preprocesamiento de diÃ¡logos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1eda14f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1eda14f",
        "outputId": "53094a04-7249-4a60-dd99-f2388a5a5fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pares totales extraÃ­dos: 35,450\n",
            "Q: ['Say , Jim , how about going for a few beers after dinner ? '\n",
            "A: ' You know that is tempting but is really not good for our fitness . '\n",
            "----------------------------------------\n",
            "Q: ' What do you mean ? It will help us to relax . '\n",
            "A: \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \"\n",
            "----------------------------------------\n",
            "Q: \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \"\n",
            "A: ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . '\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def extract_pairs(text: str):\n",
        "    turns = [l.strip() for l in text.split('\\n') if l.strip()]\n",
        "    pairs = []\n",
        "    for i in range(0, len(turns)-1, 2):\n",
        "        q, a = turns[i], turns[i+1]\n",
        "        if q and a:\n",
        "            pairs.append((q, a))\n",
        "    return pairs\n",
        "\n",
        "pairs = []\n",
        "for d in train_df['dialog'].astype(str):\n",
        "    pairs.extend(extract_pairs(d))\n",
        "\n",
        "print(f'Pares totales extraÃ­dos: {len(pairs):,}')\n",
        "\n",
        "# Mostrar ejemplo\n",
        "for q,a in pairs[:3]:\n",
        "    print('Q:', q)\n",
        "    print('A:', a)\n",
        "    print('-'*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c1f1cf9",
      "metadata": {
        "id": "1c1f1cf9"
      },
      "source": [
        "### 3.1Â |Â DivisiÃ³n Train / Val / Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c530a27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c530a27",
        "outputId": "dd08a0bc-6e56-4886-e3ed-66679911172e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 28360 | Val: 3545 | Test: 3545\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(pairs)\n",
        "total = len(pairs)\n",
        "train_cut = int(0.8*total)\n",
        "val_cut   = int(0.9*total)\n",
        "\n",
        "train_pairs = pairs[:train_cut]\n",
        "val_pairs   = pairs[train_cut:val_cut]\n",
        "test_pairs  = pairs[val_cut:]\n",
        "\n",
        "print(f'Train: {len(train_pairs)} | Val: {len(val_pairs)} | Test: {len(test_pairs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa26234",
      "metadata": {
        "id": "4fa26234"
      },
      "source": [
        "## 4Â |Â VectorizaciÃ³n de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e7d58e",
      "metadata": {
        "id": "29e7d58e"
      },
      "outputs": [],
      "source": [
        "MAX_LEN   = 50   # segÃºn rÃºbrica y anÃ¡lisis exploratorio\n",
        "MIN_FREQ  = 1\n",
        "BATCH_SZ  = 64\n",
        "\n",
        "special_new = ['<bos>', '<eos>']   # solo los que aÃ±ades\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_LEN\n",
        ")\n",
        "\n",
        "# 1) recopilar todos los textos (pregunta + respuesta)\n",
        "all_text = [txt for q, a in (train_pairs + val_pairs + test_pairs) for txt in (q, a)]\n",
        "print(\"Textos totales:\", len(all_text))\n",
        "\n",
        "# 2) adaptar â†’ crea vocabulario base con '' y '[UNK]' al frente\n",
        "vectorizer.adapt(all_text)\n",
        "\n",
        "# 3) construir vocabulario final:\n",
        "vocab_base = vectorizer.get_vocabulary()[2:]        # sin '' ni [UNK]\n",
        "new_vocab  = ['', '[UNK]'] + special_new + vocab_base\n",
        "vectorizer.set_vocabulary(new_vocab)\n",
        "\n",
        "# 4) IDs Ãºtiles\n",
        "PAD_ID, UNK_ID = 0, 1\n",
        "BOS_ID, EOS_ID = 2, 3\n",
        "VOCAB_SIZE = vectorizer.vocabulary_size()\n",
        "print(\"VOCAB_SIZE:\", VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3103afe",
      "metadata": {
        "id": "a3103afe"
      },
      "source": [
        "### 4.1Â |Â CreaciÃ³n de objetos `tf.data.Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11819b96",
      "metadata": {
        "id": "11819b96"
      },
      "outputs": [],
      "source": [
        "def format_dataset(pairs):\n",
        "    q_texts = [q for q,_ in pairs]\n",
        "    a_texts = [a for _,a in pairs]\n",
        "\n",
        "    enc = tf.cast(vectorizer(q_texts), tf.int32)\n",
        "    dec_in  = tf.cast(vectorizer(['<bos> '+t for t in a_texts]), tf.int32)\n",
        "    dec_out = tf.cast(vectorizer([t+' <eos>' for t in a_texts]), tf.int32)\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(((enc, dec_in), dec_out))\n",
        "\n",
        "\n",
        "def prepare_tf_dataset(pairs):\n",
        "    ds = format_dataset(pairs)\n",
        "    return (ds\n",
        "            .shuffle(10_000, seed=SEED)\n",
        "            .batch(BATCH_SZ)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "\n",
        "train_ds = prepare_tf_dataset(train_pairs)\n",
        "val_ds   = prepare_tf_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1a3f5f",
      "metadata": {
        "id": "6d1a3f5f"
      },
      "source": [
        "## 5Â |Â Componentes del modelo Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf50715",
      "metadata": {
        "id": "dbf50715"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, max_len, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_encoding = positional_encoding(max_len, d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Si llega un SparseTensor lo convertimos\n",
        "        if isinstance(x, tf.SparseTensor):\n",
        "            x = tf.sparse.to_dense(x)\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        return x + self.pos_encoding[:, :seq_len, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280f1143",
      "metadata": {
        "id": "280f1143"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(num_layers, d_model, num_heads, dff, input_vocab, maximum_position_encoding, rate=0.1):\n",
        "    inputs   = layers.Input(shape=(None,), name='enc_input')\n",
        "    padding_mask = layers.Lambda(lambda x: tf.cast(tf.math.equal(x, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :])(inputs)\n",
        "\n",
        "    x = layers.Embedding(input_vocab, d_model)(inputs)\n",
        "    x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    x = PositionalEncoding(maximum_position_encoding, d_model)(x)\n",
        "    x = layers.Dropout(rate)(x)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        # multiâ€‘head attention\n",
        "        attn_out = layers.MultiHeadAttention(num_heads, key_dim=d_model, dropout=rate)(x, x, attention_mask=padding_mask)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_out)\n",
        "\n",
        "        ffn_out = layers.Dense(dff, activation='relu')(x)\n",
        "        ffn_out = layers.Dense(d_model)(ffn_out)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_out)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x, name='encoder')\n",
        "\n",
        "def transformer_decoder(num_layers, d_model, num_heads, dff, target_vocab, maximum_position_encoding, rate=0.1):\n",
        "    inputs   = layers.Input(shape=(None,), name='dec_input')\n",
        "    enc_outs = layers.Input(shape=(None, d_model), name='enc_output')\n",
        "\n",
        "    look_ahead_mask = layers.Lambda(\n",
        "        lambda x: 1 - tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0))(inputs)\n",
        "    padding_mask = layers.Lambda(lambda x: tf.cast(tf.math.equal(x, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :])(inputs)\n",
        "\n",
        "    embed = layers.Embedding(target_vocab, d_model)(inputs)\n",
        "    embed *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embed = PositionalEncoding(maximum_position_encoding, d_model)(embed)\n",
        "    x = layers.Dropout(rate)(embed)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        attn1 = layers.MultiHeadAttention(num_heads, key_dim=d_model, dropout=rate)(x, x, attention_mask=look_ahead_mask)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn1)\n",
        "\n",
        "        attn2 = layers.MultiHeadAttention(num_heads, key_dim=d_model, dropout=rate)(x, enc_outs, enc_outs)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn2)\n",
        "\n",
        "        ffn = layers.Dense(dff, activation='relu')(x)\n",
        "        ffn = layers.Dense(d_model)(ffn)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn)\n",
        "\n",
        "    outputs = layers.Dense(target_vocab)(x)\n",
        "    return tf.keras.Model([inputs, enc_outs], outputs, name='decoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa9f2b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "8fa9f2b0",
        "outputId": "08c12dfd-2468-4f4d-abe3-0fadc7ccc52a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"seq2seq_transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_inputs      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_inputs      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) â”‚  \u001b[38;5;34m5,364,992\u001b[0m â”‚ encoder_inputs[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      â”‚ \u001b[38;5;34m10,223,410\u001b[0m â”‚ decoder_inputs[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
              "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m21298\u001b[0m)            â”‚            â”‚ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_inputs      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_inputs      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,364,992</span> â”‚ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,223,410</span> â”‚ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">21298</span>)            â”‚            â”‚ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,588,402\u001b[0m (59.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,588,402</span> (59.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,588,402\u001b[0m (59.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,588,402</span> (59.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "NUM_LAYERS = 4\n",
        "D_MODEL    = 128\n",
        "NUM_HEADS  = 8\n",
        "DFF        = 512\n",
        "DROPOUT    = 0.1\n",
        "\n",
        "encoder = transformer_encoder(NUM_LAYERS, D_MODEL, NUM_HEADS, DFF, VOCAB_SIZE, MAX_LEN, DROPOUT)\n",
        "decoder = transformer_decoder(NUM_LAYERS, D_MODEL, NUM_HEADS, DFF, VOCAB_SIZE, MAX_LEN, DROPOUT)\n",
        "\n",
        "enc_inputs  = layers.Input(shape=(None,), name='encoder_inputs')\n",
        "dec_inputs  = layers.Input(shape=(None,), name='decoder_inputs')\n",
        "\n",
        "enc_outs = encoder(enc_inputs)\n",
        "dec_outs = decoder([dec_inputs, enc_outs])\n",
        "\n",
        "model = tf.keras.Model([enc_inputs, dec_inputs], dec_outs, name='seq2seq_transformer')\n",
        "\n",
        "# Loss & metrics\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss_ = loss_object(y_true, y_pred)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    match = tf.cast(tf.equal(y_true, tf.cast(y_pred, tf.int32)), tf.float32)\n",
        "    mask  = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    match *= mask\n",
        "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=masked_loss, metrics=[masked_accuracy])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299e51ca",
      "metadata": {
        "id": "299e51ca"
      },
      "source": [
        "## 6Â |Â Entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d26ac3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51d26ac3",
        "outputId": "296fff6a-c42b-4665-8347-6f328c7d847c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 211ms/step - loss: 8.2271 - masked_accuracy: 0.1286 - val_loss: 5.5856 - val_masked_accuracy: 0.2358\n",
            "Epoch 2/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 151ms/step - loss: 5.0350 - masked_accuracy: 0.3252 - val_loss: 3.2634 - val_masked_accuracy: 0.6081\n",
            "Epoch 3/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 149ms/step - loss: 2.9385 - masked_accuracy: 0.6371 - val_loss: 2.0650 - val_masked_accuracy: 0.7478\n",
            "Epoch 4/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 150ms/step - loss: 1.9620 - masked_accuracy: 0.7540 - val_loss: 1.5489 - val_masked_accuracy: 0.8087\n",
            "Epoch 5/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 148ms/step - loss: 1.5037 - masked_accuracy: 0.8107 - val_loss: 1.2424 - val_masked_accuracy: 0.8473\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e901667a",
      "metadata": {
        "id": "e901667a"
      },
      "source": [
        "## 7Â |Â DecodificaciÃ³n y evaluaciÃ³n BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc46bbab",
      "metadata": {
        "id": "cc46bbab"
      },
      "outputs": [],
      "source": [
        "\n",
        "# â”€â”€â”€ IDs y vocabulario globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BOS_ID = vectorizer('<bos>').numpy()[0]\n",
        "EOS_ID = vectorizer('<eos>').numpy()[0]\n",
        "VOCAB  = vectorizer.get_vocabulary()          # lista idxâ†’token\n",
        "\n",
        "# â”€â”€â”€ GeneraciÃ³n (greedy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def generate(model, src_text: str, max_len: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    Devuelve la secuencia generada por 'model' para 'src_text'.\n",
        "    \"\"\"\n",
        "    enc_in = vectorizer([src_text])               # (1, enc_len)\n",
        "    dec_in = tf.expand_dims([BOS_ID], 0)          # (1, 1)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits = model([enc_in, dec_in], training=False)  # (1, t, vocab)\n",
        "        next_id = tf.argmax(logits[:, -1, :], axis=-1, output_type=tf.int32)\n",
        "        dec_in  = tf.concat([dec_in, tf.expand_dims(next_id, -1)], axis=-1)\n",
        "\n",
        "        if next_id[0] == EOS_ID:\n",
        "            break\n",
        "\n",
        "    # ids â†’ tokens, eliminando <bos>/<eos>\n",
        "    ids = dec_in.numpy().squeeze()\n",
        "    tokens = [VOCAB[i] for i in ids if i not in (BOS_ID, EOS_ID)]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# â”€â”€â”€ EvaluaciÃ³n BLEU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def bleu_score(model,\n",
        "               pairs,\n",
        "               max_len: int = 50) -> float:\n",
        "    \"\"\"\n",
        "    Calcula corpus-BLEU (%) sobre una lista de pares (src, ref).\n",
        "    \"\"\"\n",
        "    references, candidates = [], []\n",
        "    for src, ref in pairs:\n",
        "        pred = generate(model, src, max_len=max_len)\n",
        "        references.append([ref.split()])      # lista de refs por oraciÃ³n\n",
        "        candidates.append(pred.split())\n",
        "\n",
        "    return corpus_bleu(references, candidates) * 100\n",
        "\n",
        "# â”€â”€â”€ Ejemplo de uso â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "score = bleu_score(model, test_pairs, max_len=MAX_LEN)\n",
        "print(f\"BLEU en test: {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- obtener Ã­ndices de tokens especiales\n",
        "BOS_ID = vectorizer('<bos>').numpy()[0]\n",
        "EOS_ID = vectorizer('<eos>').numpy()[0]\n",
        "PAD_ID = vectorizer('<pad>').numpy()[0]\n",
        "\n",
        "VOCAB  = vectorizer.get_vocabulary()      # lista â†’ idxâ†’token\n",
        "\n",
        "def generate_response(model,prompt: str,\n",
        "                      max_len: int = MAX_LEN,\n",
        "                      temperature: float = 0.0) -> str:\n",
        "    enc_input = vectorizer([prompt])\n",
        "    dec_input = tf.expand_dims([BOS_ID], 0)        # (1,1)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits = model([enc_input, dec_input], training=False)[:, -1, :]\n",
        "\n",
        "        if temperature == 0.0:\n",
        "            #  tf.argmax â†’ (1,)  â†’  expand_dims â†’ (1,1)\n",
        "            next_id = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            next_id = tf.expand_dims(next_id, -1)\n",
        "        else:\n",
        "            #  tf.random.categorical ya sale (1,1)\n",
        "            next_id = tf.random.categorical(logits / temperature,\n",
        "                                            num_samples=1,\n",
        "                                            dtype=tf.int32)\n",
        "\n",
        "        dec_input = tf.concat([dec_input, next_id], axis=-1)\n",
        "\n",
        "        if next_id[0, 0] == EOS_ID:\n",
        "            break\n",
        "\n",
        "    ids = dec_input.numpy().squeeze()\n",
        "    keep = [i for i in ids if i not in (BOS_ID, EOS_ID, PAD_ID)]\n",
        "    tokens = [VOCAB[i] for i in keep]\n",
        "    return \" \".join(tokens).strip()\n",
        "\n",
        "def chat(model, temperature: float = 0.0):\n",
        "    \"\"\"\n",
        "    Bucle interactivo de consola.\n",
        "    Escribe 'salir' para terminar.\n",
        "    \"\"\"\n",
        "    print(\"=== Chat Transformer (escribe 'salir' para terminar) ===\")\n",
        "    while True:\n",
        "        user = input(\"TÃº: \").strip()\n",
        "        if user.lower() in {\"salir\", \"exit\", \"quit\"}:\n",
        "            print(\"Hasta luego ğŸ‘‹\")\n",
        "            break\n",
        "        bot = generate_response(model, user, temperature=temperature)\n",
        "        print(\"Bot:\", bot)"
      ],
      "metadata": {
        "id": "ZROLX-ljYkgK"
      },
      "id": "ZROLX-ljYkgK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6mNS8M-MMMoV",
        "outputId": "4c36e449-0d5a-4dd5-9852-e557adfabb7f"
      },
      "id": "6mNS8M-MMMoV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also also'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aLpxSApMHre",
        "outputId": "e92dde23-ddcb-478e-dd6f-939fc6f15912"
      },
      "id": "3aLpxSApMHre",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Chat Transformer (escribe 'salir' para terminar) ===\n",
            "TÃº: hello\n",
            "Bot: when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when\n",
            "TÃº: bye bye\n",
            "Bot: when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when\n",
            "TÃº: salir\n",
            "Hasta luego ğŸ‘‹\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}